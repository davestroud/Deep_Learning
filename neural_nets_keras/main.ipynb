{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/davestroud/Deep_Learning/blob/master/neural_nets_keras/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Biological Neuron</center>\n",
    "![](images/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujU6-Ng_AtY2"
   },
   "source": [
    "## “Cells that fire together, wire together”\n",
    "\n",
    "> When all the neurons in a layer are connected to every neuron in the previous layer (i.e., its input neurons), the layer is called a fully connected layer, or a **dense** layer.\n",
    "\n",
    "*the connection weight between two neurons tends to increase when they fire simultaneously.* ~ Hebb's rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcnWmzC7AtY4"
   },
   "source": [
    "## Backpropagation Algorithm\n",
    "\n",
    "> The **backpropagation** algorithm is able to compute the gradient of the network’s error with regard to every single model parameter. In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cyu6UZxlAtY5"
   },
   "source": [
    "#### Let’s run through the algorithm in a bit more detail:\n",
    "\n",
    "> It handles one mini-batch at a time (for example, containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an epoch.\n",
    "\n",
    "> Each mini-batch is passed to the network’s input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the forward pass: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "\n",
    "> Next, the algorithm measures the network’s output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "\n",
    "> Then it computes how much each output connection contributed to the error. This is done analytically by applying the **chain rule** (perhaps the most fundamental rule in calculus), which makes this step fast and precise.\n",
    "\n",
    "> The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until the algorithm reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).\n",
    "\n",
    "> Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Typical regression MLP architecture</center>\n",
    "![](images/mlp_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Typical classification MLP architecture</center>\n",
    "![](images/mlp_classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Classifier Using the Sequential API\n",
    "\n",
    "> Load the Fashion MNIST dataset. This is a drop-in replacement of the MNIST dataset.\n",
    "\n",
    "> The dataset has 70,000 greyscale images of 28 x 28 pixels each, with 10 classes.\n",
    "\n",
    "> This dataset is significantly more challenging than the MNIST dataset. For example, a simple linear model reaches around 92% accuracy on MNIST, but only around 83% on Fashion MNIST.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Samples from Fashion MNIST dataset</center>\n",
    "![](images/FMNIST_sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlxfTvARAtY6"
   },
   "outputs": [],
   "source": [
    "# Import tensorflow and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Bimy55HKBWPW",
    "outputId": "6bd33554-598f-4d5b-9d4b-354c00441187"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are using tensorflow version 2.0\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LojP762KBrC1",
    "outputId": "8abe2ac4-fe7e-4d96-f78e-f48d66970568"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datset. Note: train and test set are already split for you.\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every image is represented as a 28 x 28 array rather than a 1D array of size 784\n",
    "# Pixel intensities are represented as integers from 0 to 255, rather than floats\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of class names\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the first image in the training set:\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using the Sequential API\n",
    "\n",
    "> Here is a classification MLP with two hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that builds the neural network\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go through this code line by line:\n",
    "\n",
    "> The first line creates a Sequential model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the Sequential API.\n",
    "\n",
    "> Next, we build the first layer and add it to the model. It is a Flatten layer whose role is to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 1). This layer does not have any parameters; it is just there to do some simple preprocessing. Since it is the first layer in the model, you should specify the input_shape, which doesn’t include the batch size, only the shape of the instances. Alternatively, you could add a keras.layers.InputLayer as the first layer, setting input_shape=[28,28].\n",
    "\n",
    "> Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes the equation below: \n",
    "\n",
    " <center > <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mrow>\n",
    "    <msub>\n",
    "      <mi>h</mi>\n",
    "      <mrow>\n",
    "        <mi mathvariant=\"bold\">W</mi>\n",
    "        <mo>,</mo>\n",
    "        <mi mathvariant=\"bold\">b</mi>\n",
    "      </mrow>\n",
    "    </msub>\n",
    "    <mrow>\n",
    "      <mo>(</mo>\n",
    "      <mi mathvariant=\"bold\">X</mi>\n",
    "      <mo>)</mo>\n",
    "    </mrow>\n",
    "    <mo>=</mo>\n",
    "    <mi>&#x3D5;</mi>\n",
    "    <mrow>\n",
    "      <mo>(</mo>\n",
    "      <mi mathvariant=\"bold\">X</mi>\n",
    "      <mi mathvariant=\"bold\">W</mi>\n",
    "      <mo>+</mo>\n",
    "      <mi mathvariant=\"bold\">b</mi>\n",
    "      <mo>)</mo>\n",
    "    </mrow>\n",
    "  </mrow>\n",
    "</math> <center>\n",
    "\n",
    "> Then we add a second Dense hidden layer with 100 neurons, also using the ReLU activation function.\n",
    "\n",
    "> Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x64bca2110>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x64bc62d50>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x62fffbed0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x64bca3310>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_3') is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06714182,  0.05718173,  0.02944557, ..., -0.03960896,\n",
       "         0.04642007,  0.01297732],\n",
       "       [ 0.01088481,  0.0525094 ,  0.06691198, ...,  0.05987774,\n",
       "        -0.03304314,  0.04375952],\n",
       "       [-0.01145871, -0.00836556,  0.01767467, ..., -0.04773249,\n",
       "         0.05978806,  0.04867551],\n",
       "       ...,\n",
       "       [-0.05361543,  0.02882023,  0.00626164, ...,  0.05766775,\n",
       "         0.02289047, -0.06843333],\n",
       "       [ 0.05815023, -0.01688362, -0.06829529, ..., -0.06501156,\n",
       "         0.02588434,  0.03054632],\n",
       "       [ 0.06866685,  0.03556019, -0.02386855, ..., -0.0715955 ,\n",
       "        -0.04356875,  0.04895236]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=\"sgd\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **sparse_categorical_crossentropy** loss is used because we have sparse labels. (i.e., for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive.\n",
    "\n",
    "> **sgd** means that we will traning the model using simple Stocastic Gradient Descent.\n",
    "\n",
    "> **accuracy** is a usefull measurement, since this is a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating the Model\n",
    "\n",
    "> Now that the model is ready to be trained, we simply call its fit() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 7s 130us/sample - loss: 0.7068 - accuracy: 0.7661 - val_loss: 0.4944 - val_accuracy: 0.8374\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 6s 106us/sample - loss: 0.4876 - accuracy: 0.8307 - val_loss: 0.4534 - val_accuracy: 0.8434\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 6s 103us/sample - loss: 0.4427 - accuracy: 0.8454 - val_loss: 0.4252 - val_accuracy: 0.8552\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 6s 106us/sample - loss: 0.4173 - accuracy: 0.8532 - val_loss: 0.4113 - val_accuracy: 0.8552\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 6s 102us/sample - loss: 0.3969 - accuracy: 0.8605 - val_loss: 0.3887 - val_accuracy: 0.8646\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 6s 107us/sample - loss: 0.3802 - accuracy: 0.8656 - val_loss: 0.3689 - val_accuracy: 0.8730\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 6s 105us/sample - loss: 0.3664 - accuracy: 0.8699 - val_loss: 0.3695 - val_accuracy: 0.8704\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 6s 105us/sample - loss: 0.3551 - accuracy: 0.8744 - val_loss: 0.3912 - val_accuracy: 0.8636\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 6s 105us/sample - loss: 0.3446 - accuracy: 0.8777 - val_loss: 0.3586 - val_accuracy: 0.8760\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 6s 105us/sample - loss: 0.3354 - accuracy: 0.8813 - val_loss: 0.3575 - val_accuracy: 0.8740\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 6s 104us/sample - loss: 0.3258 - accuracy: 0.8843 - val_loss: 0.3319 - val_accuracy: 0.8854\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 6s 105us/sample - loss: 0.3176 - accuracy: 0.8871 - val_loss: 0.3447 - val_accuracy: 0.8810\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 6s 104us/sample - loss: 0.3111 - accuracy: 0.8900 - val_loss: 0.3646 - val_accuracy: 0.8654\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 6s 105us/sample - loss: 0.3041 - accuracy: 0.8914 - val_loss: 0.3368 - val_accuracy: 0.8832\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 6s 106us/sample - loss: 0.2975 - accuracy: 0.8941 - val_loss: 0.3292 - val_accuracy: 0.8824\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 6s 109us/sample - loss: 0.2915 - accuracy: 0.8962 - val_loss: 0.3196 - val_accuracy: 0.8848\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 6s 111us/sample - loss: 0.2860 - accuracy: 0.8976 - val_loss: 0.3248 - val_accuracy: 0.8842\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 6s 111us/sample - loss: 0.2788 - accuracy: 0.9004 - val_loss: 0.3375 - val_accuracy: 0.8762\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 6s 109us/sample - loss: 0.2747 - accuracy: 0.9012 - val_loss: 0.3120 - val_accuracy: 0.8876\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 6s 109us/sample - loss: 0.2693 - accuracy: 0.9046 - val_loss: 0.3172 - val_accuracy: 0.8846\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.2640 - accuracy: 0.9057 - val_loss: 0.3100 - val_accuracy: 0.8854\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 6s 113us/sample - loss: 0.2594 - accuracy: 0.9071 - val_loss: 0.3105 - val_accuracy: 0.8860\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 6s 108us/sample - loss: 0.2556 - accuracy: 0.9081 - val_loss: 0.3088 - val_accuracy: 0.8878\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 6s 108us/sample - loss: 0.2509 - accuracy: 0.9101 - val_loss: 0.3052 - val_accuracy: 0.8896\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.2465 - accuracy: 0.9119 - val_loss: 0.3008 - val_accuracy: 0.8898\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.2425 - accuracy: 0.9131 - val_loss: 0.3205 - val_accuracy: 0.8810\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 6s 110us/sample - loss: 0.2391 - accuracy: 0.9139 - val_loss: 0.3021 - val_accuracy: 0.8922\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 6s 111us/sample - loss: 0.2346 - accuracy: 0.9158 - val_loss: 0.3173 - val_accuracy: 0.8802\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 6s 117us/sample - loss: 0.2313 - accuracy: 0.9170 - val_loss: 0.3140 - val_accuracy: 0.8804\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 6s 115us/sample - loss: 0.2269 - accuracy: 0.9199 - val_loss: 0.2965 - val_accuracy: 0.8904\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 30, \n",
    "                   validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
